# ğŸ”„ Dual-Track Development Guide
## Parallel Glue + Databricks Risk Engine Migration

**Use Case:** Risk engine running 24/7 on Glue while migrating to Databricks in parallel

---

## ğŸ¯ Your Scenario

### **The Challenge**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION (Running 24/7)                â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         AWS Glue Risk Engine (Python Libraries)      â”‚  â”‚
â”‚  â”‚  â€¢ Can't stop (runs daily)                          â”‚  â”‚
â”‚  â”‚  â€¢ Team makes changes (columns, tables, logic)      â”‚  â”‚
â”‚  â”‚  â€¢ Changes communicated after ~1 week delay         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â†“                                 â”‚
â”‚                   [Needs to sync]                          â”‚
â”‚                           â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      Databricks Risk Engine (Migration Target)      â”‚  â”‚
â”‚  â”‚  â€¢ Migrated Glue code                               â”‚  â”‚
â”‚  â”‚  â€¢ + NEW Databricks-specific features (Process D)   â”‚  â”‚
â”‚  â”‚  â€¢ Must sync Glue changes without losing new code   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Requirements**

1. âœ… **Glue continues running** - Can't stop production risk engine
2. âœ… **Sync Glue changes** - Get updates from Glue team (weekly)
3. âœ… **Preserve Databricks code** - Don't overwrite new features
4. âœ… **Detect conflicts** - Warn when both sides changed same file
5. âœ… **Track provenance** - Know what came from where

---

## ğŸš€ Solution: Dual-Track Manager

### **How It Works**

```python
from glue2lakehouse.dual_track import DualTrackManager

# Initialize manager
manager = DualTrackManager(
    glue_source="glue_risk_engine/",      # Your Glue repository
    databricks_target="databricks_risk_engine/"  # Your Databricks repository
)

# STEP 1: Initial migration (one-time)
result = manager.initial_migration(catalog_name="production")

# STEP 2: Every week - sync Glue changes
result = manager.sync_glue_changes()

# STEP 3: Protect Databricks-specific code
manager.mark_as_databricks_native("databricks_risk_engine/process_d.py")
```

---

## ğŸ“‹ Step-by-Step Workflow

### **Week 1: Initial Migration**

```python
from glue2lakehouse.dual_track import DualTrackManager

# Initialize
manager = DualTrackManager(
    glue_source="/path/to/glue_risk_engine/",
    databricks_target="/path/to/databricks_risk_engine/"
)

# Perform initial migration
print("ğŸš€ Starting initial migration...")
result = manager.initial_migration(catalog_name="production")

if result.success:
    print(f"âœ… Migrated {result.files_synced} files")
    print(f"   All files marked as originated from Glue")
else:
    print(f"âŒ Errors: {result.errors}")
```

**What Happens:**
- âœ… All Glue Python files migrated to Databricks
- âœ… Each file marked with `# SOURCE: GLUE` header
- âœ… File hashes stored for change detection
- âœ… State saved to `.dual_track_state.json`

---

### **Week 2-N: Building New Features in Databricks**

```python
# You build new Process D in Databricks (not in Glue)
# File: databricks_risk_engine/process_d.py

# Mark it as Databricks-native (protected from sync)
manager.mark_as_databricks_native("process_d.py")

# You can also protect specific code regions in existing files
manager.protect_code_region(
    file_path="process_a.py",
    start_line=100,  # Your new Databricks-specific code
    end_line=150
)
```

**What Happens:**
- âœ… `process_d.py` marked with `# SOURCE: DATABRICKS_NATIVE`
- âœ… Will NOT be overwritten by Glue sync
- âœ… Protected regions marked with `# PROTECTED: START/END`

---

### **Weekly: Sync Glue Changes**

```python
# Glue team made changes (new column, table, logic)
# You get notified after a week

# First: Dry-run to see what would change
print("ğŸ” Checking for Glue changes (dry-run)...")
result = manager.sync_glue_changes(dry_run=True)

print(f"ğŸ“Š Sync Preview:")
print(f"   Files to sync: {result.files_synced}")
print(f"   Files protected: {result.files_protected}")
print(f"   Conflicts: {len(result.conflicts)}")

# If looks good, apply the sync
if input("Apply changes? (y/n): ") == 'y':
    result = manager.sync_glue_changes(dry_run=False)
    
    if result.success:
        print(f"âœ… Synced {result.files_synced} Glue changes")
        print(f"ğŸ”’ Protected {result.files_protected} Databricks files")
        
        if result.conflicts:
            print(f"\nâš ï¸  Conflicts detected:")
            for conflict in result.conflicts:
                print(f"   - {conflict['file']}: {conflict['reason']}")
```

**What Happens:**
- âœ… Detects which Glue files changed
- âœ… Re-migrates only changed files
- âœ… **Skips** Databricks-native files
- âœ… **Skips** protected regions
- âœ… Reports conflicts for manual review

---

## ğŸ”’ Protection Strategies

### **Strategy 1: Databricks-Native Files**

Use when **entire file** is Databricks-specific:

```python
# Example: New process created only in Databricks
manager.mark_as_databricks_native("process_d.py")
manager.mark_as_databricks_native("databricks_utils.py")
manager.mark_as_databricks_native("new_feature/")
```

### **Strategy 2: Protected Code Regions**

Use when **part of file** has Databricks-specific code:

```python
# Example: Added Databricks-specific logging to existing file
manager.protect_code_region(
    file_path="process_a.py",
    start_line=50,   # Your Databricks addition
    end_line=75
)

# File will have markers:
# Line 50: # PROTECTED: START - DO NOT OVERWRITE
# ... your Databricks code ...
# Line 75: # PROTECTED: END
```

---

## ğŸ“Š Monitoring & Status

### **Check Sync Status**

```python
status = manager.get_sync_status()

print(f"ğŸ“Š Sync Status:")
print(f"   Initialized: {status['initialized']}")
print(f"   Last sync: {status['last_sync']}")
print(f"   Total files: {status['total_files']}")
print(f"   Databricks-native files: {status['databricks_native_files']}")
print(f"   Protected regions: {status['protected_regions']}")
print(f"   Sync history: {status['sync_history_count']} syncs")
```

### **List Databricks-Native Files**

```python
databricks_files = manager.list_databricks_native_files()

print(f"ğŸ”’ Databricks-native files ({len(databricks_files)}):")
for file in databricks_files:
    print(f"   - {file}")
```

### **List Conflicts**

```python
conflicts = manager.list_conflicts()

if conflicts:
    print(f"âš ï¸  Conflicts from last sync:")
    for conflict in conflicts:
        print(f"   File: {conflict['file']}")
        print(f"   Reason: {conflict['reason']}")
        print(f"   Action: Manual review required")
```

---

## ğŸ¯ Real-World Example

### **Your Risk Engine Scenario**

```python
# ============================================================================
# Initial Setup (Week 1)
# ============================================================================
from glue2lakehouse.dual_track import DualTrackManager

manager = DualTrackManager(
    glue_source="/prod/glue_risk_engine/",
    databricks_target="/dev/databricks_risk_engine/"
)

# Migrate all Glue code
result = manager.initial_migration(catalog_name="production_risk")
print(f"âœ… Initial migration: {result.files_synced} files")

# ============================================================================
# Week 2-4: Building New Features in Databricks
# ============================================================================

# New process D (only in Databricks)
manager.mark_as_databricks_native("risk_calculations/process_d.py")

# Enhanced monitoring (Databricks-specific)
manager.mark_as_databricks_native("monitoring/databricks_metrics.py")

# Protected region in existing file
manager.protect_code_region(
    file_path="risk_calculations/process_a.py",
    start_line=200,  # Databricks optimization
    end_line=250
)

# ============================================================================
# Week 5: Glue Team Notifies Changes
# ============================================================================
# "We added new column 'risk_score_v2' to customers table"
# "Updated process_b.py logic"

# Check what would sync
result = manager.sync_glue_changes(dry_run=True)

print(f"\nğŸ“Š Sync Preview:")
print(f"   Files to update: {result.files_synced}")
print(f"      - process_b.py (Glue changed)")
print(f"      - data_loader.py (new column handling)")
print(f"   ")
print(f"   Files protected: {result.files_protected}")
print(f"      - process_d.py (Databricks-native)")
print(f"      - monitoring/databricks_metrics.py (Databricks-native)")
print(f"   ")
print(f"   Files with protected regions: 1")
print(f"      - process_a.py (lines 200-250 protected)")

# Apply sync
result = manager.sync_glue_changes(dry_run=False)

if result.success:
    print(f"\nâœ… Sync complete!")
    print(f"   âœ… Glue changes applied: {result.glue_changes_applied}")
    print(f"   ğŸ”’ Databricks code preserved: {result.databricks_code_preserved}")

# ============================================================================
# Week 6-N: Repeat Weekly
# ============================================================================
# Every week when Glue team notifies changes:
# 1. Run sync_glue_changes(dry_run=True) to preview
# 2. Review conflicts if any
# 3. Run sync_glue_changes(dry_run=False) to apply
```

---

## ğŸ“ File Markers

### **Glue-Originated Files**

```python
# SOURCE: GLUE
"""
Risk calculation process A
Originated from Glue, synced weekly
"""

def calculate_risk(data):
    # This code syncs from Glue
    ...
```

### **Databricks-Native Files**

```python
# SOURCE: DATABRICKS_NATIVE
"""
Process D - New feature
Built only in Databricks, never sync from Glue
"""

def new_databricks_feature():
    # This code is Databricks-specific
    ...
```

### **Protected Regions**

```python
# SOURCE: GLUE
"""
Risk calculation process A
"""

def calculate_risk(data):
    # This code syncs from Glue
    risk = data.amount * 0.1
    
    # PROTECTED: START - DO NOT OVERWRITE
    # Databricks-specific optimization
    if spark.conf.get("spark.databricks.delta.optimizeWrite.enabled"):
        risk = risk * optimization_factor
    # PROTECTED: END
    
    return risk  # This code syncs from Glue
```

---

## âš ï¸ Handling Conflicts

### **Scenario: Both Teams Changed Same File**

```python
# Glue team: Changed process_a.py
# Databricks team: Also modified process_a.py

result = manager.sync_glue_changes()

if result.conflicts:
    print("âš ï¸  CONFLICT DETECTED")
    print(f"File: process_a.py")
    print(f"Reason: Has protected regions but Glue also changed")
    print(f"\nAction required:")
    print(f"1. Review Glue changes manually")
    print(f"2. Merge changes carefully")
    print(f"3. Update file manually")
```

### **Resolution Options**

1. **Manual Merge** (Recommended)
   ```bash
   # Compare versions
   diff glue_risk_engine/process_a.py databricks_risk_engine/process_a.py
   
   # Manually merge changes
   # Keep protected regions, apply Glue updates to non-protected areas
   ```

2. **Force Glue Version** (Loses Databricks changes)
   ```python
   # Only if you want to discard Databricks changes
   manager.sync_glue_changes(force_overwrite=True)  # Not implemented by default
   ```

3. **Keep Databricks Version** (Ignore Glue changes)
   ```python
   # Mark as Databricks-native to stop syncing
   manager.mark_as_databricks_native("process_a.py")
   ```

---

## ğŸ”„ Automated Weekly Sync Script

```python
#!/usr/bin/env python3
"""
weekly_sync.py - Automated Glue to Databricks sync script
Run this every Monday after Glue team notification
"""

from glue2lakehouse.dual_track import DualTrackManager
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)

def weekly_sync():
    """Perform weekly Glue sync."""
    
    print(f"\n{'='*70}")
    print(f"  WEEKLY GLUE SYNC - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*70}\n")
    
    # Initialize manager
    manager = DualTrackManager(
        glue_source="/prod/glue_risk_engine/",
        databricks_target="/dev/databricks_risk_engine/"
    )
    
    # Step 1: Check status
    status = manager.get_sync_status()
    print(f"ğŸ“Š Current Status:")
    print(f"   Last sync: {status['last_sync']}")
    print(f"   Total files: {status['total_files']}")
    print(f"   Databricks-native: {status['databricks_native_files']}")
    
    # Step 2: Dry-run to preview
    print(f"\nğŸ” Checking for Glue changes...")
    result = manager.sync_glue_changes(dry_run=True)
    
    print(f"\nğŸ“‹ Sync Preview:")
    print(f"   Files to sync: {result.files_synced}")
    print(f"   Files protected: {result.files_protected}")
    print(f"   Conflicts: {len(result.conflicts)}")
    
    if result.conflicts:
        print(f"\nâš ï¸  CONFLICTS DETECTED:")
        for conflict in result.conflicts:
            print(f"   - {conflict['file']}: {conflict['reason']}")
        print(f"\nâŒ Manual review required before syncing")
        return
    
    # Step 3: Apply sync
    if result.files_synced > 0:
        print(f"\nğŸš€ Applying sync...")
        result = manager.sync_glue_changes(dry_run=False)
        
        if result.success:
            print(f"\nâœ… Sync complete!")
            print(f"   Glue changes applied: {result.glue_changes_applied}")
            print(f"   Databricks code preserved: {result.databricks_code_preserved}")
        else:
            print(f"\nâŒ Sync failed: {result.errors}")
    else:
        print(f"\nâœ¨ No Glue changes detected. All up to date!")
    
    print(f"\n{'='*70}\n")

if __name__ == "__main__":
    weekly_sync()
```

**Schedule it:**
```bash
# Add to crontab for weekly Monday 9am sync
0 9 * * 1 cd /dev/databricks_risk_engine && python3 weekly_sync.py
```

---

## ğŸ“Š State File (`.dual_track_state.json`)

```json
{
  "version": "1.0.0",
  "initialized": true,
  "last_sync": "2026-02-13T10:30:00",
  "glue_file_hashes": {
    "risk_calculations/process_a.py": "abc123...",
    "risk_calculations/process_b.py": "def456...",
    "data_loader.py": "ghi789..."
  },
  "databricks_native_files": [
    "risk_calculations/process_d.py",
    "monitoring/databricks_metrics.py"
  ],
  "protected_regions": {
    "risk_calculations/process_a.py": [
      {"start": 200, "end": 250}
    ]
  },
  "sync_history": [
    {
      "type": "initial_migration",
      "timestamp": "2026-01-15T10:00:00",
      "files_migrated": 45
    },
    {
      "type": "sync",
      "timestamp": "2026-02-13T10:30:00",
      "files_synced": 3,
      "conflicts": 0
    }
  ]
}
```

---

## âœ… Best Practices

### **1. Initial Migration**
- âœ… Do once at project start
- âœ… Review migrated code before committing
- âœ… Test thoroughly in Databricks

### **2. Mark Databricks Code Immediately**
- âœ… As soon as you create new Databricks-specific code
- âœ… Better to over-protect than under-protect
- âœ… Can always unmark later if needed

### **3. Weekly Sync Routine**
- âœ… Set fixed day/time for sync (e.g., Monday 9am)
- âœ… Always run dry-run first
- âœ… Review conflicts manually
- âœ… Communicate with Glue team if issues

### **4. Use Version Control**
- âœ… Commit before sync
- âœ… Review diff after sync
- âœ… Can rollback if needed

### **5. Document Databricks-Specific Code**
- âœ… Add comments explaining why it's Databricks-specific
- âœ… Makes it clear to team members
- âœ… Helps during conflict resolution

---

## ğŸ‰ Summary

Your dual-track development is now managed! The framework:

âœ… **Syncs Glue changes** - Automatically detect and apply Glue updates  
âœ… **Protects Databricks code** - Never overwrites your new features  
âœ… **Detects conflicts** - Warns when manual merge needed  
âœ… **Tracks everything** - State file records all changes  
âœ… **Easy to use** - Simple API, automated workflows  

**Your risk engine can run 24/7 on Glue while you build the future on Databricks!** ğŸš€
